# Copyright (c) 2025 XBXyftx
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
51CTOå¼€æºæŠ€æœ¯ç¤¾åŒºçˆ¬è™«
çˆ¬å–åœ°å€: https://ost.51cto.com/postlist
"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from bs4 import BeautifulSoup
import logging
import time
import random
from typing import List, Dict, Optional, Callable
from datetime import datetime
import hashlib

logger = logging.getLogger(__name__)

class Cto51Crawler:
    """51CTOå¼€æºæŠ€æœ¯ç¤¾åŒºçˆ¬è™«"""

    def __init__(self, headless: bool = True):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        """
        self.base_url = "https://ost.51cto.com/postlist"
        self.headless = headless
        self.driver: Optional[webdriver.Chrome] = None

        # ç”¨æˆ·ä»£ç†æ± ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]

    def _init_driver(self) -> webdriver.Chrome:
        """åˆå§‹åŒ–Selenium WebDriver"""
        try:
            chrome_options = Options()

            # ä½¿ç”¨æ— å¤´æ¨¡å¼
            if self.headless:
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--disable-gpu')

            # åçˆ¬è™«ç­–ç•¥
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            # éšæœºUser-Agent
            user_agent = random.choice(self.user_agents)
            chrome_options.add_argument(f'user-agent={user_agent}')

            # çª—å£å¤§å°
            chrome_options.add_argument('--window-size=1920,1080')

            # å…¶ä»–ä¼˜åŒ–é€‰é¡¹
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-notifications')
            chrome_options.add_argument('--disable-popup-blocking')

            # åˆ›å»ºdriver
            driver = webdriver.Chrome(options=chrome_options)

            # è®¾ç½®éšå¼ç­‰å¾…
            driver.implicitly_wait(10)

            # æ‰§è¡ŒCDPå‘½ä»¤éšè—webdriverç‰¹å¾
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5]
                    });
                    Object.defineProperty(navigator, 'languages', {
                        get: () => ['zh-CN', 'zh', 'en']
                    });
                '''
            })

            logger.info("âœ… Selenium WebDriveråˆå§‹åŒ–æˆåŠŸ")
            return driver

        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–WebDriverå¤±è´¥: {e}")
            raise

    def _random_delay(self, min_seconds: float = 1.0, max_seconds: float = 3.0):
        """éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)

    def _human_like_scroll(self, driver: webdriver.Chrome):
        """æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º"""
        try:
            # éšæœºæ»šåŠ¨è·ç¦»
            scroll_distance = random.randint(300, 800)
            driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
            self._random_delay(0.5, 1.5)

            # æœ‰æ—¶å€™å‘ä¸Šæ»šä¸€ç‚¹
            if random.random() > 0.7:
                driver.execute_script(f"window.scrollBy(0, -{random.randint(100, 300)});")
                self._random_delay(0.3, 0.8)
        except Exception as e:
            logger.warning(f"âš ï¸ æ¨¡æ‹Ÿæ»šåŠ¨å¤±è´¥: {e}")

    def _generate_article_id(self, url: str) -> str:
        """æ ¹æ®URLç”Ÿæˆå”¯ä¸€ID"""
        return hashlib.md5(url.encode()).hexdigest()[:16]

    def _parse_article_detail(self, driver: webdriver.Chrome, article_url: str) -> Optional[Dict]:
        """
        è§£ææ–‡ç« è¯¦æƒ…é¡µ

        Args:
            driver: WebDriverå®ä¾‹
            article_url: æ–‡ç« URL

        Returns:
            æ–‡ç« è¯¦æƒ…å­—å…¸
        """
        try:
            # è®¿é—®æ–‡ç« é¡µé¢
            driver.get(article_url)
            self._random_delay(2, 4)

            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º
            self._human_like_scroll(driver)

            # è·å–é¡µé¢HTML
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')

            # æå–æ ‡é¢˜
            title = ""
            title_selectors = [
                'h1.article-title',
                'h1.post-title',
                '.article-header h1',
                'h1'
            ]
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    break

            if not title:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ ‡é¢˜: {article_url}")
                return None

            # æå–æ—¥æœŸ
            date = ""
            date_selectors = [
                '.post-meta time',
                '.article-meta time',
                'time',
                '.publish-time',
                '.date'
            ]
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date = date_elem.get_text(strip=True)
                    # å°è¯•ä»datetimeå±æ€§è·å–
                    if not date and date_elem.get('datetime'):
                        date = date_elem['datetime']
                    break

            # å¦‚æœæ²¡æœ‰æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¶é—´
            if not date:
                date = datetime.now().strftime("%Y-%m-%d")

            # æå–æ–‡ç« å†…å®¹
            content_blocks = []
            content_selectors = [
                '.article-content',
                '.post-content',
                '.content',
                'article'
            ]

            content_container = None
            for selector in content_selectors:
                content_container = soup.select_one(selector)
                if content_container:
                    break

            if content_container:
                # éå†å†…å®¹å…ƒç´ 
                for element in content_container.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'img', 'pre', 'code', 'video']):
                    if element.name == 'img':
                        # å›¾ç‰‡å…ƒç´ 
                        img_src = element.get('src') or element.get('data-src')
                        if img_src:
                            # è¡¥å…¨ç›¸å¯¹URL
                            if img_src.startswith('//'):
                                img_src = 'https:' + img_src
                            elif img_src.startswith('/'):
                                img_src = 'https://ost.51cto.com' + img_src
                            content_blocks.append({
                                "type": "image",
                                "value": img_src
                            })
                    elif element.name == 'video':
                        # è§†é¢‘å…ƒç´ 
                        video_src = element.get('src')
                        if video_src:
                            if video_src.startswith('//'):
                                video_src = 'https:' + video_src
                            elif video_src.startswith('/'):
                                video_src = 'https://ost.51cto.com' + video_src
                            content_blocks.append({
                                "type": "video",
                                "value": video_src
                            })
                    elif element.name in ['pre', 'code']:
                        # ä»£ç å—
                        code_text = element.get_text(strip=True)
                        if code_text:
                            content_blocks.append({
                                "type": "code",
                                "value": code_text
                            })
                    else:
                        # æ–‡æœ¬å…ƒç´ 
                        text = element.get_text(strip=True)
                        if text:
                            content_blocks.append({
                                "type": "text",
                                "value": text
                            })

            # å¦‚æœæ²¡æœ‰è§£æåˆ°å†…å®¹å—ï¼Œå°è¯•è·å–æ•´ä½“æ–‡æœ¬
            if not content_blocks and content_container:
                full_text = content_container.get_text(strip=True)
                if full_text:
                    # æŒ‰æ®µè½åˆ†å‰²
                    paragraphs = [p.strip() for p in full_text.split('\n') if p.strip()]
                    for para in paragraphs[:10]:  # æœ€å¤šå–å‰10æ®µ
                        content_blocks.append({
                            "type": "text",
                            "value": para
                        })

            # æå–æ‘˜è¦ï¼ˆå–å‰3ä¸ªæ–‡æœ¬å—ï¼‰
            summary = ""
            text_blocks = [block['value'] for block in content_blocks if block['type'] == 'text']
            if text_blocks:
                summary = ' '.join(text_blocks[:3])[:200] + '...'

            # æ„å»ºæ–‡ç« æ•°æ®
            article_data = {
                "id": self._generate_article_id(article_url),
                "title": title,
                "date": date,
                "url": article_url,
                "content": content_blocks if content_blocks else [{"type": "text", "value": "æš‚æ— å†…å®¹"}],
                "category": "å¼€æºæŠ€æœ¯",
                "summary": summary or title[:100],
                "source": "51CTOå¼€æºç¤¾åŒº",
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            }

            logger.info(f"âœ… æˆåŠŸè§£ææ–‡ç« : {title}")
            return article_data

        except TimeoutException:
            logger.error(f"âŒ é¡µé¢åŠ è½½è¶…æ—¶: {article_url}")
            return None
        except Exception as e:
            logger.error(f"âŒ è§£ææ–‡ç« è¯¦æƒ…å¤±è´¥: {e}", exc_info=True)
            return None

    def crawl_articles(
        self,
        max_pages: int = 3,
        batch_callback: Optional[Callable[[List[Dict]], None]] = None
    ) -> List[Dict]:
        """
        çˆ¬å–51CTOå¼€æºç¤¾åŒºæ–‡ç« 

        Args:
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            batch_callback: æ‰¹é‡å›è°ƒå‡½æ•°ï¼Œç”¨äºåˆ†æ‰¹å¤„ç†æ•°æ®

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        all_articles = []

        try:
            # åˆå§‹åŒ–driver
            self.driver = self._init_driver()

            logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–51CTOå¼€æºç¤¾åŒºï¼Œç›®æ ‡é¡µæ•°: {max_pages}")

            # è®¿é—®é¦–é¡µ
            self.driver.get(self.base_url)
            self._random_delay(3, 5)

            current_page = 1

            while current_page <= max_pages:
                try:
                    logger.info(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µ")

                    # ç­‰å¾…æ–‡ç« åˆ—è¡¨åŠ è½½
                    WebDriverWait(self.driver, 15).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "ul.infinite-list"))
                    )

                    # æ¨¡æ‹Ÿäººç±»æµè§ˆ
                    self._human_like_scroll(self.driver)
                    self._random_delay(2, 4)

                    # è·å–å½“å‰é¡µé¢çš„æ–‡ç« åˆ—è¡¨
                    try:
                        article_elements = self.driver.find_elements(
                            By.CSS_SELECTOR,
                            "ul.infinite-list li.infinite-list-item"
                        )

                        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(article_elements)} ç¯‡æ–‡ç« ")

                        page_articles = []

                        for idx, article_elem in enumerate(article_elements, 1):
                            try:
                                # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
                                link_elem = article_elem.find_element(By.CSS_SELECTOR, "a")
                                article_url = link_elem.get_attribute('href')

                                if not article_url:
                                    continue

                                # è¡¥å…¨URL
                                if article_url.startswith('/'):
                                    article_url = 'https://ost.51cto.com' + article_url

                                logger.info(f"  ğŸ“– [{idx}/{len(article_elements)}] æ­£åœ¨è§£æ: {article_url}")

                                # è§£ææ–‡ç« è¯¦æƒ…
                                article_data = self._parse_article_detail(self.driver, article_url)

                                if article_data:
                                    page_articles.append(article_data)
                                    all_articles.append(article_data)

                                # è¿”å›åˆ—è¡¨é¡µ
                                self.driver.back()
                                self._random_delay(2, 4)

                                # é‡æ–°ç­‰å¾…åˆ—è¡¨åŠ è½½
                                WebDriverWait(self.driver, 10).until(
                                    EC.presence_of_element_located((By.CSS_SELECTOR, "ul.infinite-list"))
                                )

                            except (NoSuchElementException, StaleElementReferenceException) as e:
                                logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–‡ç« å…ƒç´ : {e}")
                                continue
                            except Exception as e:
                                logger.error(f"âŒ å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                                continue

                        # æ‰¹é‡å›è°ƒ
                        if batch_callback and page_articles:
                            try:
                                batch_callback(page_articles)
                                logger.info(f"âœ… ç¬¬{current_page}é¡µæ•°æ®å·²é€šè¿‡å›è°ƒå¤„ç†")
                            except Exception as e:
                                logger.error(f"âŒ æ‰¹é‡å›è°ƒå¤±è´¥: {e}")

                        # æŸ¥æ‰¾å¹¶ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
                        if current_page < max_pages:
                            try:
                                # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
                                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                                self._random_delay(1, 2)

                                # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                                next_button_selectors = [
                                    "button.btn-next",
                                    ".pagination .next",
                                    "a.next",
                                    "button:contains('ä¸‹ä¸€é¡µ')",
                                    ".el-pagination button.btn-next"
                                ]

                                next_button = None
                                for selector in next_button_selectors:
                                    try:
                                        next_button = self.driver.find_element(By.CSS_SELECTOR, selector)
                                        if next_button and next_button.is_enabled():
                                            break
                                    except NoSuchElementException:
                                        continue

                                if next_button and next_button.is_enabled():
                                    logger.info("ğŸ”„ ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®")

                                    # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_button)
                                    self._random_delay(0.5, 1)

                                    # ç‚¹å‡»
                                    next_button.click()
                                    self._random_delay(3, 5)

                                    current_page += 1
                                else:
                                    logger.info("â„¹ï¸ æ²¡æœ‰æ›´å¤šé¡µé¢äº†")
                                    break

                            except Exception as e:
                                logger.warning(f"âš ï¸ ç¿»é¡µå¤±è´¥: {e}")
                                break
                        else:
                            break

                    except TimeoutException:
                        logger.error("âŒ ç­‰å¾…æ–‡ç« åˆ—è¡¨è¶…æ—¶")
                        break

                except Exception as e:
                    logger.error(f"âŒ å¤„ç†ç¬¬{current_page}é¡µæ—¶å‡ºé”™: {e}")
                    break

            logger.info(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
            return all_articles

        except Exception as e:
            logger.error(f"âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return all_articles

        finally:
            # å…³é—­æµè§ˆå™¨
            if self.driver:
                try:
                    self.driver.quit()
                    logger.info("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
                except Exception as e:
                    logger.error(f"âŒ å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")


def main():
    """æµ‹è¯•å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    crawler = Cto51Crawler(headless=True)
    articles = crawler.crawl_articles(max_pages=2)

    print(f"\næ€»å…±çˆ¬å– {len(articles)} ç¯‡æ–‡ç« ")
    for i, article in enumerate(articles, 1):
        print(f"{i}. {article['title']} - {article['url']}")


if __name__ == "__main__":
    main()
